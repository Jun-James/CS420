## 分类指标

### 精度

  分类正确的样本占样本总数的比例
  Acc = $\frac{TP + TN}{TP + TN + FP + FN}$

### 准确率

  预测为1的样本中标签为1的比例
  Precision = $\frac{TP}{TP + FP}$

### 召回率

  标签为1的样本中预测为1的比例
  Recall = $\frac{TP}{TP + FN}$


### F1度量

  召回率和准确率的权衡
  F1= $\frac{2 * Precision * Recall}{Precision + Recall}$


### ROC曲线

  AUC面积能判断分类器的排序能力

# 分类指标
## 评估指标
根据预测结果为1或0及标签为1或0，我们可以将预测结果和标签组合成四种形式。预测结果为1，标签为1，设为True Positive；预测为1，标签为0设为False Positive；预测为0，标签为1设为False Negative；预测为0，标签为0设为True Negative。这样我们来定义几个评价指标：

**精度**（Accuracy）:分类正确的样本数占样本总数的比例可以表示为：

$$
\mathrm{Acc}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}}
$$

**准确率**（Precision）:**预测**为1的样本中**标签**为1的比例

$$
\text { Prec }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
$$

**召回率**（Recall）:**标签**为1的样本中**预测**为1的比例

$$
\text{Rec}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
$$

准确率和召回率之间的关系可以表示为：

$$
\hat{y}=\left\{\begin{array}{ll}
1, &  p_{\theta} (y=1 | x)>h \\
0, & \text { otherwise }
\end{array}\right.
$$

$p_{\theta}(y=1 | x)$ 表示在参数为 $\theta$ 的模型参数下，预测为1的样本中，标签为1的概率。h为设定阈值。

阈值越高，代表准确度越高，召回率越低。阈值极端值=0.99；
阈值越低，代表准确度越低，召回率越高。阈值极端值=0

**那么如何来权衡准确率和召回
率呢？**

**F1 度量** ：值越大，代表模型表现越佳。

$$
\mathrm{F} 1=\frac{2 \times \text { Precision } \times \mathrm{Recall}}{\text { Precision }+\mathrm{Recall}}
$$


**AUC评价法（ROC（Receiver operating characteristic）曲线下面积）**

这是一种基于排序的度量方法。可以理解为用描点法，瞄出概率从高到低排列中，每个预测概率下，真正率和假正率组成的点坐标，进而得到ROC曲线，计算其包含的面积后，即可得到AUC值。
（注：具体绘图后会展示的比较清楚）
AUC=1时是完美预测，0.5时是随机预测，一般0.8以上就可以认为预测结果不错。

 
